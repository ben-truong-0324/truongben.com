<p>Here's the big idea: get your model to the lowest point in the loss map. Can you do it?</p>
<br>
<p>Let's say, your team leader wants to optimize for accuracy for the sprint, but the BA gave you a hot tip it's gonna be recall, but finance insists ROI depends on precision; while the DL library just care about CE loss or *insert buzzword fn* loss. The darnest thing is, each combination of hyperparameters will point to a different point on the map in each map. Scale that up to thousands, millions of different features, and you have yourself a proper mess.</p>

<div class="landscape-grid" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px; margin: 20px 0;">

    <div class="landscape-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
        <img src="loss_land1.png" alt="Loss landscape for cross-entropy loss" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        <p style="margin-top: 8px; font-size: 0.9em; font-style: italic;">
            <strong>Map 1: Cross-Entropy Loss.</strong> The model's default map for loss. The lowest point is where the training algorithm naturally wants to go via its implementation of backprop and criterion as CE loss.
        </p>
    </div>

    <div class="landscape-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
        <img src="loss_land2.png" alt="Loss landscape for accuracy" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        <p style="margin-top: 8px; font-size: 0.9em; font-style: italic;">
            <strong>Map 2: Accuracy.</strong> The team lead's map. Notice the 'lowest point' has shifted to a different coordinate of hyperparameters. Optimizing for loss doesn't guarantee the best accuracy, though we can argue accuracy is basically CE loss... operationally.
        </p>
    </div>

    <div class="landscape-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
        <img src="loss_land3.png" alt="Loss landscape for recall" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        <p style="margin-top: 8px; font-size: 0.9em; font-style: italic;">
            <strong>Map 3: Recall.</strong> The BA's map. A completely different terrain. The ideal hyperparameters for best recall and grabbing as many true positives as possible are somewhere else entirely. You want to minimize opportunity cost or no?
        </p>
    </div>

    <div class="landscape-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
        <img src="loss_land4.png" alt="Loss landscape for precision" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
        <p style="margin-top: 8px; font-size: 0.9em; font-style: italic;">
            <strong>Map 4: Precision.</strong> Finance's map. To minimize false positives, we must navigate to yet another optimal coordinate. You want to minimize soon-to-default credit lines or no?
        </p>
    </div>
</div>

<p>
    The challenge is clear: each set of hyperparameters takes you to a single spot sure, but its "performance" is judged differently based on the flavor of the eval_perf of the day. Finding the "best" model is thus heavily about tradeoffs - but let's assume that's been resolved. Let's say the current challenge is whether we could even converge towards the global minimum of a single map. Sure you can brute force it, but that's easily exponential O(). So you do randomized search instead to at least PAC converge (which is still better than intractability.)
</p>
<hr>
<h2>Wait, what's PAC?</h2>
<p>It'd take a literal forever to flip every rock on the map to find the best spot. This is where the principle of Probably Approximately Correct (PAC) learning comes in. Instead of demanding a guaranteed, perfect solution, it's better to just go with PAC where you're happy with a solution that is "good enough" (approximately correct) with a very high likelihood (probably). Randomized search is how you bring this philosophy to life. By sampling random spots from the enormous problem space, we aren't trying to find the single best needle in an infinite haystack; instead, we are efficiently casting a wide net, making it highly probable that we will discover a high-performing region and land upon an "approximately correct" solution without the impossible cost of brute-force.</p>

<hr>
<h2>It's ok to just PAC and RO</h2>
<p>This is where Randomized Optimization (RO) strategies come in.
</p>
<p>
    Real quick - why RO? As you may have noticed, loss maps are often rugged and non-convex. If your starting point was on some random hill and you descend to the neighborhood well, then you declare that well to be the deepest spot in the world while asserting that the Mariana trench is fake news, that'd be suboptimal. That is, the model may hit a local minima and unknowingly got trapped in a <strong>basin of attraction</strong>.
</p>
    <p>
    By randomly restarting our starting point, we can "escape" local basins and hopefully explore a better spot (and eventually stumble upon the basin of the global minimum.) We can now frame this process as exploration vs exploitation. Whenver we start over, we "exploit" the local slopes and descend towards the lowest point possible in the neighborhood. Once there, we will do some quick mental math to decide whether this will be a good stopping point or to continue to "explore" the world and restart again. For a quick recap, lets consider 4 different RO strategies.
    </p>

    <div class="ro-strategy-grid" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px; margin: 20px 0;">
    
        <div class="ro-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
            <img src="ro_rhc.png" alt="Conceptual visualization of Randomized Hill Climbing" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
            <p style="margin-top: 8px; font-size: 0.9em;">
                <strong>Randomized Hill Climbing:</strong> Takes the next best random step, but can easily get stuck on the first "peak" it finds (or "basin", depending if we're multiplying by -1 or not.) Restart randomly on the map until hit quota.
            </p>
        </div>
    
        <div class="ro-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
            <img src="ro_sa.png" alt="Conceptual visualization of Simulated Annealing" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
            <p style="margin-top: 8px; font-size: 0.9em;">
                <strong>Simulated Annealing:</strong> Either explore or exploit depending on a temp metric that "cools" down to explore less and less. Inspired by hot metal with flexible particle lattice that "shifts" toward a more robust arrangement with each hammering.
            </p>
        </div>
    
        <div class="ro-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
            <img src="ro_ga.png" alt="Conceptual visualization of a Genetic Algorithm" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
            <p style="margin-top: 8px; font-size: 0.9em;">
                <strong>Genetic Algorithm:</strong> Evolves a population of solutions (aka hyperparameter permutations,) combining the best to create supposedly better offspring. 
            </p>
        </div>
    
        <div class="ro-item" style="flex: 1 1 45%; max-width: 45%; min-width: 280px; text-align: center;">
            <img src="ro_mimic.png" alt="Conceptual visualization of MIMIC" style="width: 100%; height: auto; border: 1px solid #ddd; border-radius: 4px;">
            <p style="margin-top: 8px; font-size: 0.9em;">
                <strong>MIMIC:</strong> Builds a statistical probability map of where the best solutions are, then samples from it. Depicted is StableDiffusion's understanding of MIMIC - surprisingly close to that of the average ML student
            </p>
        </div>
    
    </div>

<table class="table" style="width:100%; border-collapse: collapse;">
    <thead>
        <tr >
            <th style="padding: 8px; border: 1px solid #ddd; text-align: left;">RO Hyperparameter</th>
            <th style="padding: 8px; border: 1px solid #ddd; text-align: left;">Algorithm</th>
            <th style="padding: 8px; border: 1px solid #ddd; text-align: left;">Effect</th>
            <th style="padding: 8px; border: 1px solid #ddd; text-align: left;">Neural Net Equivalence</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="padding: 8px; border: 1px solid #ddd;">Population Size</td>
            <td style="padding: 8px; border: 1px solid #ddd;">GA, MIMIC</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Search breadth / Greediness</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Batch Size</td>
        </tr>
        <tr>
            <td style="padding: 8px; border: 1px solid #ddd;">Mutation Rate</td>
            <td style="padding: 8px; border: 1px solid #ddd;">GA</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Increases diversity/exploration</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Regularization / Dropout Rate</td>
        </tr>
        <tr>
            <td style="padding: 8px; border: 1px solid #ddd;">Initial Temperature</td>
            <td style="padding: 8px; border: 1px solid #ddd;">SA</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Initial exploration rate</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Initial Learning Rate</td>
        </tr>
        <tr>
            <td style="padding: 8px; border: 1px solid #ddd;">Cooling Schedule</td>
            <td style="padding: 8px; border: 1px solid #ddd;">SA, MIMIC</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Shift from exploration to exploitation</td>
            <td style="padding: 8px; border: 1px solid #ddd;">Learning Rate Decay</td>
        </tr>
    </tbody>
</table>
<hr>
<h2>Conclusion</h2>
<p>
    Since canvassing the entire map to guarantee the best solve is intractable, we make do with PAC. The question now becomes how do we structure our RO on exploit vs explore, how to systematically "avoid" local basins and as such sample enough of the big wide world.
</p>
<p>
    There is no single "best" algorithm. RHC and SA offer speed, but at the risk of getting stuck. GA offers a more robust and powerful search, but at a higher computational cost. Not to mention if the eval_perf metric changes, you have to redo everything. Of course, if data shift or concept shift happens, you have to redo everything. The key is to understand the nature of the problem space and navigate tradeoffs as you set a direction for your search space, and delegate to the oncall. 
</p>


<div class="charts-container" style="margin-top: 2em;">

    <figure style="margin-bottom: 3em;">
        <img src="graphs_Knapsack_by_psize_13.png" alt="Chart showing Knapsack problem fitness score by algorithm as problem size increases." style="width: 100%; height: auto; max-width: 700px; display: block; margin: 0 auto; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; font-size: 0.9em;">
            <b>Figure 1: Knapsack Performance vs. Problem Size.</b> This chart illustrates how each algorithm's fitness score scales as the complexity of the Knapsack problem (the number of items) grows.
        </figcaption>
    </figure>

    <figure style="margin-bottom: 3em;">
        <img src="monte_TSP - Fitness by Algo.png" alt="Box plot chart showing the distribution of fitness scores for the TSP after many Monte Carlo runs." style="width: 100%; height: auto; max-width: 700px; display: block; margin: 0 auto; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; font-size: 0.9em;">
            <b>Figure 2: TSP Fitness Distribution (Monte Carlo Runs).</b> By running the simulation many times, we can see the consistency of each algorithm. Note the wide variance for some, indicating a high sensitivity to the random starting conditions.
        </figcaption>
    </figure>

    <figure style="margin-bottom: 3em;">
        <img src="val_sa_by_cooling_tsp.png" alt="Chart validating Simulated Annealing performance on the TSP with different cooling schedules." style="width: 100%; height: auto; max-width: 700px; display: block; margin: 0 auto; border: 1px solid #ddd;">
        <figcaption style="margin-top: 8px; font-size: 0.9em;">
            <b>Figure 3: Impact of Cooling Schedule on Simulated Annealing.</b> This validation curve shows how a single hyperparameter—the cooling schedule in SA—can drastically affect the final solution's quality and consistency.
        </figcaption>
    </figure>

</div>

<p>Tldr: there is no free lunch.</p>
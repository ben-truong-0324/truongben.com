<article>
  <h2>Developing Production-Grade Agents with Google's ADK</h2>
  <p>
    Moving AI agents from prototype to a production-grade system requires a
    methodological approach to governance, reliability, and scale. The Google
    Agent Development Kit (ADK) provides a framework for this engineering
    discipline.
  </p>
  <p>
    This guide outlines the five key stages of the agent development lifecycle
    using the ADK, from initial construction to scalable deployment.
  </p>

  <div style="margin: 2rem 0">
    <img
      src="agentic_workflow_adk.png"
      alt="A demo agentic workflow for compliance."
      style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1)"
    />
    <p><small>A sample agentic workflow for compliance</small></p>
  </div>

  <h2>Step 1: Foundational Agent Construction</h2>
  <p>
    The initial phase involves configuring the environment (e.g., API keys for
    Gemini) and instantiating a single agent. This is accomplished by defining
    an <strong>`Agent`</strong> class with its core properties: the
    <strong>`model`</strong> (e.g., Gemini), an <strong>`instruction`</strong>
    (the system prompt), and a list of available <strong>`tools`</strong>
    (such as `Google Search`). The agent is executed using an
    <strong>`InMemoryRunner`</strong>, which orchestrates the prompt, the
    agent's reasoning, and the invocation of tools to generate a response.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-introduction-to-agents"
        target="_blank"
        >Introduction to Agents</a
      >
    </li>
  </ul>

  <h2>Step 2: Implementing Tools and Interoperability</h2>
  <p>
    This stage introduces two advanced patterns for agent tools. The first is
    the <strong>Model Context Protocol (MCP)</strong>, an open standard for
    interoperability. Instead of writing custom API clients, an agent uses an
    <strong>`McpToolset`</strong> to connect to an external MCP server (e.g.,
    for GitHub or Kaggle), which standardizes access to its available tools.
  </p>
  <p>
    The second pattern is <strong>Long-Running Operations (LROs)</strong>,
    which are critical for human-in-the-loop (HITL) workflows, such as
    requiring approval before a costly action. This is implemented by injecting
    a <strong>`ToolContext`</strong> into the tool function. The function calls
    <strong>`tool_context.request_confirmation()`</strong> to pause the
    agent's execution and signal for external input.
  </p>
  <p>
    To manage the pause, the agent must be wrapped in a resumable
    <strong>`App`</strong> with <strong>`ResumabilityConfig`</strong>. The
    application's <strong>`Runner`</strong> is then responsible for detecting
    the <code>adk_request_confirmation</code> event, awaiting the human
    decision, and resuming the workflow by passing the approval and the original
    <strong>`invocation_id`</strong> back to the runner.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-agent-tools-and-interoperability-with-mcp"
        target="_blank"
        >Agent Tools & Interoperability with Model Context Protocol (MCP)</a
      >
    </li>
  </ul>

  <h2>Step 3: Context Engineering for Statefulness</h2>
  <p>
    Context engineering is implemented by distinguishing between two components:
    <strong>`Sessions`</strong> for short-term, single-conversation history,
    and <strong>`Memory`</strong> for long-term, persistent knowledge across
    conversations. A <strong>`MemoryService`</strong> (e.g.,
    `InMemoryMemoryService` for development or `VertexAiMemoryBankService` for
    production) is provided to the <strong>`Runner`</strong> alongside the
    `SessionService`.
  </p>
  <p>
    This enables two core processes. First, <strong>ingestion</strong>, where
    session data is explicitly persisted to the long-term store using
    <strong>`memory_service.add_session_to_memory()`</strong>. Second,
    <strong>retrieval</strong>, where the agent uses built-in tools like
    <strong>`load_memory`</strong> (reactive search) or
    <strong>`preload_memory`</strong> (proactive search) to query this
    knowledge. The ingestion step can be automated by using
    <strong>`after_agent_callback`</strong> to persist the session after
    every turn. Production-grade services also perform
    <strong>memory consolidation</strong>, extracting key facts from raw logs.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-context-engineering-sessions-and-memory"
        target="_blank"
        >Context Engineering: Sessions & Memory</a
      >
    </li>
  </ul>

  <h2>Step 4: Agent Quality and Observability</h2>
  <div style="margin: 2rem 0">
    <img
      src="count-invocation-plugin.png"
      alt="Plugins and callbacks and how they integrate into ADK runners"
      style="width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1)"
    />
    <p><small>Plugins and callbacks and how they integrate into ADK runners</small></p>
  </div>
  <p>
    Ensuring production-grade reliability involves two disciplines: reactive
    <strong>observability</strong> (debugging what went wrong) and proactive
    <strong>agent evaluation</strong> (catching failures before they
    happen).
  </p>

  <h3>Observability with Plugins and Callbacks</h3>
  <p>
    Unlike traditional software, agents can fail mysteriously. Observability
    provides visibility into the agent's internal decision-making process
    through logs, traces, and metrics. For local debugging, the ADK Web UI
    (run with <strong>`adk web --log_level DEBUG`</strong>) offers a
    detailed, real-time view of LLM prompts and tool calls.
  </p>
  <p>
    For automated or production environments, this logic is captured using
    <strong>Plugins</strong>. A Plugin is a module that hooks into the
    agent's lifecycle using specific <strong>Callbacks</strong> (e.g.,
    <strong>`before_agent_callback`</strong>,
    <strong>`after_tool_callback`</strong>,
    <strong>`on_model_error_callback`</strong>). These callbacks can
    execute custom code, such as logging, at critical points.
  </p>
  <p>
    Instead of building this from scratch, the ADK provides a built-in
    <strong>`LoggingPlugin`</strong>. By registering this plugin with the
    <strong>`Runner`</strong>, all agent activity—user messages, LLM
    requests, tool calls, and errors—is automatically logged, providing a
    complete execution trace for production monitoring.
  </p>

  <h3>Agent Evaluation</h3>
  <p>
    While observability is reactive, <strong>Agent Evaluation</strong>
    provides a proactive approach to identify quality degradations. This is
    essential because agents are non-deterministic and must be assessed
    beyond simple "happy path" tests.
  </p>
  <p>
    The evaluation process involves running a set of test cases defined in an
    <strong>`*.evalset.json`</strong> file, which contains the user prompt,
    the expected final response, and the expected tool-call trajectory
    (including parameters). The <strong>`adk eval`</strong> CLI command
    executes these tests and compares the agent's actual output against these
    expectations.
  </p>
  <p>
    Pass/fail thresholds are set in a <strong>`test_config.json`</strong>
    file, which defines criteria for two key metrics: the
    <strong>Response Match Score</strong> (text similarity of the final
    answer) and the <strong>Tool Trajectory Score</strong> (correctness of
    tool calls and arguments). This systematic regression testing catches
    deviations in both functional behavior and response quality. More
    advanced testing can use <strong>User Simulation</strong>, where an LLM
    dynamically generates prompts to test agent robustness.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-agent-observability-and-logging"
        target="_blank"
        >Agent Observability & Logging</a
      >
    </li>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-agent-quality"
        target="_blank"
        >Agent Quality</a
      >
    </li>
  </ul>

  <h2>Step 5: Productionization and Deployment</h2>
  <p>
    The final stage, productionization, is facilitated by the
    <strong>Agent2Agent (A2A) Protocol</strong>, an open standard for
    multi-agent collaboration. This is ideal for systems that cross network,
    language, or organizational boundaries. An ADK agent can be
    <strong>exposed</strong> as a service using the <strong>`to_a2a()`</strong>
    function, which wraps it in a server and auto-generates an
    <strong>`agent card`</strong>. This JSON document, served at a
    well-known path, acts as a formal contract by describing the agent's
    capabilities (skills) and endpoints.
  </p>
  <p>
    Conversely, a remote agent can be <strong>consumed</strong> by using the
    <strong>`RemoteA2aAgent`</strong> class. This client-side proxy reads the
    remote agent's card and makes it available as a local sub-agent. ADK
    transparently handles all protocol-level communication (e.g., HTTP POSTs
    to task endpoints), allowing for complex, heterogeneous systems to be built
    from specialized, independent agents. The system can then be deployed to a
    scalable platform like <strong>Vertex AI Agent Engine</strong>.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-prototype-to-production"
        target="_blank"
        >Prototype to Production</a
      >
    </li>
  </ul>
</article>
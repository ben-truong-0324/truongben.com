<article>
  <h2>Developing Production-Grade Agents with Google's ADK</h2>
  <p>
    Moving AI agents from prototype to a production-grade system requires a
    methodological approach to governance, reliability, and scale. The Google
    Agent Development Kit (ADK) provides a framework for this engineering
    discipline.
  </p>
  <p>
    This guide outlines the five key stages of the agent development lifecycle
    using the ADK, from initial construction to scalable deployment.
  </p>

  <h2>Step 1: Foundational Agent Construction</h2>
  <p>
    The initial phase involves configuring the environment (e.g., API keys for
    Gemini) and instantiating a single agent. This is accomplished by defining
    an <strong>`Agent`</strong> class with its core properties: the
    <strong>`model`</strong> (e.g., Gemini), an <strong>`instruction`</strong>
    (the system prompt), and a list of available <strong>`tools`</strong>
    (such as `Google Search`). The agent is executed using an
    <strong>`InMemoryRunner`</strong>, which orchestrates the prompt, the
    agent's reasoning, and the invocation of tools to generate a response.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-introduction-to-agents"
        target="_blank"
        >Introduction to Agents</a
      >
    </li>
  </ul>

  <h2>Step 2: Implementing Tools and Interoperability</h2>
  <p>
    This stage introduces two advanced patterns for agent tools. The first is
    the <strong>Model Context Protocol (MCP)</strong>, an open standard for
    interoperability. Instead of writing custom API clients, an agent uses an
    <strong>`McpToolset`</strong> to connect to an external MCP server (e.g.,
    for GitHub or Kaggle), which standardizes access to its available tools.
  </p>
  <p>
    The second pattern is <strong>Long-Running Operations (LROs)</strong>,
    which are critical for human-in-the-loop (HITL) workflows, such as
    requiring approval before a costly action. This is implemented by injecting
    a <strong>`ToolContext`</strong> into the tool function. The function calls
    <strong>`tool_context.request_confirmation()`</strong> to pause the
    agent's execution and signal for external input.
  </p>
  <p>
    To manage the pause, the agent must be wrapped in a resumable
    <strong>`App`</strong> with <strong>`ResumabilityConfig`</strong>. The
    application's <strong>`Runner`</strong> is then responsible for detecting
    the <code>adk_request_confirmation</code> event, awaiting the human
    decision, and resuming the workflow by passing the approval and the original
    <strong>`invocation_id`</strong> back to the runner.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-agent-tools-and-interoperability-with-mcp"
        target="_blank"
        >Agent Tools & Interoperability with Model Context Protocol (MCP)</a
      >
    </li>
  </ul>

  <h2>Step 3: Context Engineering for Statefulness</h2>
  <p>
    Context engineering is implemented by distinguishing between two components:
    <strong>`Sessions`</strong> for short-term, single-conversation history,
    and <strong>`Memory`</strong> for long-term, persistent knowledge across
    conversations. A <strong>`MemoryService`</strong> (e.g.,
    `InMemoryMemoryService` for development or `VertexAiMemoryBankService` for
    production) is provided to the <strong>`Runner`</strong> alongside the
    `SessionService`.
  </p>
  <p>
    This enables two core processes. First, <strong>ingestion</strong>, where
    session data is explicitly persisted to the long-term store using
    <strong>`memory_service.add_session_to_memory()`</strong>. Second,
    <strong>retrieval</strong>, where the agent uses built-in tools like
    <strong>`load_memory`</strong> (reactive search) or
    <strong>`preload_memory`</strong> (proactive search) to query this
    knowledge. The ingestion step can be automated by using
    <strong>`after_agent_callback`</strong> to persist the session after
    every turn. Production-grade services also perform
    <strong>memory consolidation</strong>, extracting key facts from raw logs.
  </p>
  <ul>
    <li>
      <strong>Whitepaper:</strong>
      <a
        href="https://www.kaggle.com/whitepaper-context-engineering-sessions-and-memory"
        target="_blank"
        >Context Engineering: Sessions & Memory</a
      >
    </li>
  </ul>

  <h2>Step 4: Agent Quality and Observability</h2>
  <p>
    Assuring agent quality requires a robust evaluation framework built on
    observability. This framework consists of three pillars:
    <strong>Logs</strong> (event diaries), <strong>Traces</strong> (narratives
    of decision-making), and <strong>Metrics</strong> (health reports). This
    data enables debugging and establishes a continuous feedback loop using
    scalable methods like <strong>LLM-as-a-Judge</strong> or HITL evaluation.
  </p>
  <ul>
    <li><strong>Whitepaper:</strong> Agent Quality</li>
  </ul>

  <h2>Step 5: Productionization and Deployment</h2>
  <p>
    The final stage transitions the agent from a local prototype to an
    enterprise-grade service. This involves using the
    <strong>Agent2Agent (A2A) Protocol</strong> to orchestrate communication
    between multiple independent agents. The completed system is then packaged
    as a scalable service and deployed to a managed platform, such as
    <strong>Vertex AI Agent Engine</strong> on Google Cloud.
  </p>
  <ul>
    <li><strong>Whitepaper:</strong> Prototype to Production</li>
  </ul>
</article>